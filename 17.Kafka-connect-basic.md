# Kafka Connect Basic 

## 개요

- Kafka Connector는 외부 시스템과 kafka 브로커를 연결해주는 프레임워크이다. 
- 데이터베이스, 파일 시스템, noSQL (key-value store, documentdb 등)과 같은 외부 시스템과 연동을 수행할 수 있다. 
- Kafka Connector는 컴포넌트화 되어 바로 사용할 수 있다. 외부 시스템에서 Kafka 토픽으로 부터 데이터를 가져온다.
- 이미 존재하는 커넥터를 이용할 수 있고, 자체 커넥터를 구성할 수 있다. 

- 소스 커넥터
  - 소스 커넥터는 시스템으로 부터 데이터를 수집하고, 소스 시스템은 데이터베이스, 스트림 테이블, 메시지 브로커 등이 있다. 
  - 소스 커넥터는 또한 애플리케이션 서버에서 Kafka 주제로 메트릭을 수집하여 짧은 대기 시간으로 스트림 데이터를 만들 수 있다. 

- 싱크 커넥터
  - 싱크 컨넥터는 Kafka 토픽으로 부터 다른 시스템으로 데이터를 전달한다. 데이터베이스, Hadoop, 등과 같은 데이터저장소로 이관할 수 있다. 

## 지원기능 

- 외부 시스템과 연결 지원 
- 분산 모드, 스탠드 얼론 모드 지원
- REST API 지원 
- 자동 offset 관리 
- 기본적으로 분산, 확장성 지원
- 스트리밍, 배치 통합
- 각 메시지의 변환 지원 

## kafka 설치 

- kafka 설치는 [install-multi-broker](03.kafka_install_multi_broker_docker_compose.md) 참조

## 테스트하기 

- 소스 커넥터
  - 소스 커넥터는 File source connector 를 이용한다. 
- 싱크 커넥터
  - 싱크 커넥터역시 File sink connector 를 이용한다. 

### Source Connector 설정하기 

- 소스 커넥터를 구성하기 위해서 connect-file-source.properties 를 다음과 같이 작성한다. 

```go
name=local-file-source
connector.class=FileStreamSource
tasks.max=1
topic=connect-test
file=/etc/tutorial/source.txt
```

- name: 커넥터를 위한 이름을 지정한다. 
- connector.class: 커넥터의 종유를 나타내며, FileStreamSource로 지정했다. 
- tasks.max: 소스 커넥터를 병렬로 수행할 인스턴스 개수를 나타낸다. 
- topic: 커넥터와 연동할 브로커의 topic을 지정한다. 
- file: 소스 파일을 지정한다. 

<br/>
- test.txt 파일을 다음과 같이 작성하자. 

```go
Hello this is example of connector.
My name is Schooldevops
Apple
Watermelon
Orange
melon
lemon
```

### Sink Connector 설정하기 

- 이제 싱크 커넥터를 만들자. connect-file-sink.properties 파일을 생성하고 다음과 같이 작성하자. 

```go
name=local-file-sink
connector.class=FileStreamSink
tasks.max=1
topic=connect-test
file=/etc/tutorial/sink.txt
```

- name: 커넥터를 위한 이름을 지정한다. 
- connector.class: 커넥터의 종유를 나타내며, FileStreamSink로 지정했다. 
- tasks.max: 소스 커넥터를 병렬로 수행할 인스턴스 개수를 나타낸다. 
- topic: 커넥터와 연동할 브로커의 topic을 지정한다. 
- file: 대상 파일을 지정한다.

### Worker Config 설정하기

- 마지막으로 worker를 설정한다. 
- 이는 커넥터를 브로커와 연결하고, 소스로 부터 데이터를 읽고, 싱크를 수행한다. 
- connect-standalone.properties 파일을 생성하고 다음과 같이 작업한다. 

```go
bootstrap.servers=kafka-1:29092,kafka-2:39092,kafka-3:49092
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false
offset.storage.file.filename=/tmp/connect.offsets
offset.flush.interval.ms=10000
plugin.path=/share/java
```

- bootstrap.servers: kafka 브로커의 서버 주소이다. 
- key.converter: 키 변환 컨버터
- value.converter: 값 변환 컨버터
- key.converter.schemas.enable: 키 변환 스키마 적용 여부 
- value.converter.schemas.enable: 값 변환 스키마 적용 여부 
- offset.storage.file.filename: 오프셋 정보를 저장할 파일을 지정한다. 
- offset.flush.interval.ms: 플러시 인터벌시간 밀리초 단위
- plugin.path: 플러그인 경로 

### 커넥터 실행하기 

- 이제 다음과 같이 실행하자. 

```go
docker-compose exec kafka-1 bash
```

- 컨테이너 환경에서 다음 커맨드를 실행한다. 

```go
connect-standalone /etc/tutorial/connect-standalone.properties /etc/tutorial/connect-file-source.properties /etc/tutorial/connect-file-sink.properties
```

### 컨슈머로 전송된 내역 확인하기

```go
kafka-console-consumer --bootstrap-server kafka-1:29092,kafka-2:29093,kafka-3:29094 --topic connect-test --from-beginning
"Hello this is example of connector."
"My name is Schooldevops"
"Apple"
"Watermelon"
"Orange"
"melon"
"lemon"
```

- 위와 같이 consumer 를 실행하여 메시지를 확인했다. 

### sink.txt 파일 확인하기. 

```go
My name is Schooldevops
Apple
Watermelon
Orange
melon
Hello this is example of connector.
lemon

```

- 전달된 데이터가 동일하게 포함되어 있다. 
- 그런데 순서가 보장되어 있지 않음을 확인하자. (이 부분은 파티션이 분리되어 전달되었기 때문에 데이터 순서가 바뀔 수 있다.)

### source 파일에 로그 추가하기. 

```go
Hello this is example of connector.
My name is Schooldevops
Apple
Watermelon
Orange
melon
lemon

-------
Java
Go
Python
```

- 이후 sink.txt 파일에는 추가한 내역이 그대로 추가된다. 

```go
...생략

lemon

-------
Java
Go
Python
```

- 중요한 포인트는 source는 마지막 내용을 추가하고, 라인을 다음 라인으로 넘겨야 정상적으로 마지막 메시지가 싱크된다. 
- 파일 커넥터는 뉴라인으로 구분하므로 반드시 확인해야한다. 

## REST API.

- 위 내용은 커넥터를 커맨드로 실행했다. 
- 그러나 kafka 커넥터는 REST API 를 제공하며, 이를 통해서 커넥터를 설정하고 메시지를 전달할 수 있는 방법이 있다. 

- REST API: http://localhost:8083 이 가능하다. 
  - GET /connectors: 사용중인 모든 커넥터 리스트를 확인할 수 있다. 
  - GET /connectors/[name]: 특정 커넥터의 이름을 반환한다. 
  - POST /connectors: 새로운 커넥터를 생성한다. 요청된 본문은 문자열 이름 필드와 커넥터 구성 매개변수가 있는 개체 구성 필드가 포함된 JSON 객체이어야한다. 
  - GET /connectors/[name]/status: 현재 커넥터의 상태를 보여준다. 실행중인지, 실패했는지 또는 일시 중지 되었는지 포함하고 있다. 
  - DELETE /connectors/[name]: 커넥터를 삭제한다. 모든 태스크는 gracefully 하게 정지된다.설정도 삭제된다.  
  - GET /connector-plugins: Kafka 커넥터 클러스터에 설치된 커넥터 플러그인을 반환한다. 

```go
curl http://localhost:8083/connectors

["local-file-source","local-file-sink"]
```

- 위와 같이 우리가 등록한 source, sink 커넥터 2개가 존재함을 알 수 있다. 

```go
root@61c386f71551:/# curl http://localhost:8083/connectors/local-file-source/status

{"name":"local-file-source","connector":{"state":"RUNNING","worker_id":"172.19.0.5:8083"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"172.19.0.5:8083"}],"type":"source"}
```

- 커넥터의 상세 정보를 확인할 수 있다. 

## 분산 모드로 실행하기 

- 독립 실행형 모드는 개발 및 테스트 용도로 사용한다. 
- 카프카의 분산된 모드의 완전한 사용을 원한다면 분산 모드로 실행이 필요하다. 

### 커넥트 시작하기 

- 분산 모든에 대한 설정은 connect-distributed.properties 를 이용한다. 
- 매개 변수 내용은 standalone 모드와 유사하다.

```go
bootstrap.servers=kafka-1:29092,kafka-2:39092,kafka-3:49092
group.id=connect-cluster
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true

offset.storage.topic=connect-offsets
offset.storage.replication.factor=1

config.storage.topic=connect-configs
config.storage.replication.factor=1

status.storage.topic=connect-status
status.storage.replication.factor=1

offset.flush.interval.ms=10000

plugin.path=/share/java
```

- bootstrap.servers: 브로커로 접속할 서버목록을 기술한다.
- group.id: 커넥트 클러스터의 그룹 아이디를 지정한다. 
- key.converter: 키 컨버터를 지정한다. JsonConverter으로 지정하였다. 
- value.converter: 값 컨버터를 지정한다. JsonConverter으로 지정하였다.
- key.converter.schemas.enable: 키에 대한 컨버터 스키마를 사용할지 여부를 지정한다. 
- value.converter.schemas.enable: 값에 대한 컨버터 스키마를 사용할지 여부를 지정한다. 
- offset.storage.topic: 오프셋 저장 토픽을 지정한다. 
- offset.storage.replication.factor: 오프셋 복제 계수를 지정한 멀티 클러스터는 브로커 개수에 맞게 조정이 필요하다. 
- config.storage.topic: 설정 값을 저장할 토픽을 지정한다. 
- config.storage.replication.factor: 설정 값의 복제 계수를 지정한다. 
- status.storage.topic: 상태 정보를 저장할 토픽을 지정한다. 
- status.storage.replication.factor: 상태 정보의 복제 계수를 지정한다. 
- offset.flush.interval.ms: 자동 커밋 오프셋의 커밋 인터벌을 지정한다. 10초에 1번 수행하게 설정했다. 
- plugin.path: 플러그인 경로를 지정한다. 

### 커넥터 수행하기 

- 이제 커넥터를 실행해보자. 

```go
docker-compose exec kafka-1 bash
```

```go
connect-distributed /etc/tutorial/connect-distributed.properties
```

### 커넥터 생성하기 REST API 이용


#### source 설정 json 생성하기

- connect-file-source.json 파일을 생성하고 다음과 같이 작성한다. 
  
```go
{
    "name": "local-file-source",
    "config": {
        "connector.class": "FileStreamSource",
        "tasks.max": 1,
        "file": "/etc/tutorial/source.txt",
        "topic": "connect-distributed"
    }
}
```

#### REST API source 요청하기 

```go
curl -d @"/etc/tutorial/connect-file-source.json" -H "Content-Type: application/json" -X POST http://localhost:8083/connectors
```

#### sink 설정 json 생성하기 

- connect-file-sink.json 파일을 생성하고 다음과 같이 작성한다. 

```go
{
    "name": "local-file-sink",
    "config": {
        "connector.class": "FileStreamSink",
        "tasks.max": 1,
        "file": "/etc/tutorial/dist-source.txt",
        "topics": "connect-distributed"
    }
}
```

#### REST API sink 요청하기 

```go
curl -d @"/etc/tutorial/connect-file-sink.json" -H "Content-Type: application/json" -X POST http://localhost:8083/connectors
```

### 전송내역 확인하기 

```go
root@d55ad1cfc12c:/etc/tutorial# cat dist-source.txt 
Hello this is example of connector.
My name is Schooldevops
Apple
Watermelon
Orange
melon
lemon

-------
Java
Go
Python
```

- 위와 같이 우리가 원하는 대로 source 내역이 sink 되었다. 

## 데이터 변환하기 

### 변환을 위한 지원 기능 

- 각 메시지를 변환하기 위한 방법을 이용하기 위해서 다음과 같은 기능을 지원한다. 
  - InsertField: 각 정적 데이터 혹은 레코드 메타 데이터를 사용하여 필드 추가
  - ReplaceField: Filter 혹은 이름을 변경한다. 
  - MaskField: 필드를 유형에 대한 유효한 null 값으로 교체한다. (0 또는 문자열)
  - HoistField: 전체 이벤트를 구조체 또는 맵 내부의 단일 필드로 래핑한다. 
  - ExtractField: 구조체 및 맵에서 특정 필드를 추출하고 이 필드만 결과에 포함한다. 
  - SetSchemaMetadata: 스키마 이름 혹은 버젼을 변경한다. 
  - TimestampRouter: 원본 토픽과 타임스탬프를 기반으로 레코드의 토픽을 변경한다. 
  - RegexRouter: 원본 토픽을 기반으로 레코드의 토픽을 변경한다. 스트링을 교체하고 정규 표현식으로 수행한다.
- 변환 파라미터
  - transforms: 변환에 대한 콤마로 분리된 별칭 목록
  - transforms.$alias.type: 변환에 대한 클래스 이름 
  - transforms.$alias.$transformationSpecificConfig: 각 변환에 대한 설정 

### Transformer 적용하기 

- JSON 구조로 메시지를 래핑하기
- 해당 구조체 필드 추가하기

- 변환을 적용하기 전에 connect-distributed.properties 를 수정하여 스키마 없는 JSON 을 사용하도록 Connect를 구성하자. 

```go
key.converter.schemas.enable=false
value.converter.schemas.enable=false
```

- 이후 Connect 를 분산 모드로 다시 수행한다. 


```go
connect-distributed /etc/tutorial/connect-distributed-for-transform.properties
```

#### Transform 을 수행하는 source 파일 생성하기 

- connect-file-source-transform.json 파일을 다음과 같이 작성하자. 

```go
{
    "name": "local-file-source",
    "config": {
        "connector.class": "FileStreamSource",
        "tasks.max": 1,
        "file": "/etc/tutorial/source.txt",
        "topic": "connect-test",
        "transforms": "MakeMap,InsertSource",
        "transforms.MakeMap.type": "org.apache.kafka.connect.transforms.HoistField$Value",
        "transforms.MakeMap.field": "line",
        "transforms.InsertSource.type": "org.apache.kafka.connect.transforms.InsertField$Value",
        "transforms.InsertSource.static.field": "data_source",
        "transforms.InsertSource.static.value": "test-file-source"
    }
}
```

- POST REST API를 다음과 같이 작성한다. 

```go
curl -d @/etc/tutorial/connect-file-source-transform.json -H "Content-Type: application/json" -X POST http://localhost:8083/connectors
```

- 이 처리를 수행하고 나면 다음과 같이 처리된다. 

```gi
{line=My name is Schooldevops, data_source=test-file-source}
{line=Apple, data_source=test-file-source}
{line=Watermelon, data_source=test-file-source}
{line=Orange, data_source=test-file-source}
{line=melon, data_source=test-file-source}
{line=lemon, data_source=test-file-source}
{line=, data_source=test-file-source}
{line=-------, data_source=test-file-source}
```

## Connector 설치하기. 

- 특정 커넥터는 이미 kafka 배포판에 포함되어 있다. 
  - ElasticSearch, HDFS, JDBC, AWS S3 등과 같은 것들이 있다. 
- 다른 커넥터들은 [Confluent Hub](https://www.confluent.io/hub/) 원하는 커넥터를 찾을 수 있다. 

### confluent-hub 클라이언트 설치 

- 우선 커넥터를 설치하기 위해서 confluent-hub 를 설치해야한다. 
- https://docs.confluent.io/5.0.4/connect/managing/confluent-hub/client.html 에서 시스템 환경에 맞는 클라이언트를 설치하자. 
- 

### Connector 설치하기 

- Confluent Hub에서 커넥터를 찾아 설치할 수 있다. 
- MongoDB sink 커넥터를 다음과 같이 설치하자. 

```go
confluent-hub install hpgrahsl/kafka-connect-mongodb:1.4.0
```

```go
Do you want to install this into /usr/share/confluent-hub-components? (yN) Do you want to install this into /usr/share/confluent-hub-components? (yN) y

 
Component's license: 
The Apache License, Version 2.0 
https://www.apache.org/licenses/LICENSE-2.0 
I agree to the software license agreement (yN) y

You are about to install 'kafka-connect-mongodb' from Hans-Peter Grahsl, as published on Confluent Hub. 
Do you want to continue? (yN) y

Downloading component Kafka Connect MongoDB Sink 1.4.0, provided by Hans-Peter Grahsl from Confluent Hub and installing into /usr/share/confluent-hub-components 
Detected Worker's configs: 
  1. Standard: /etc/kafka/connect-distributed.properties 
  2. Standard: /etc/kafka/connect-standalone.properties 
  3. Used by Connect process with PID : /etc/tutorial/connect-distributed-for-transform.properties 
Do you want to update all detected configs? (yN) y

Adding installation directory to plugin path in the following files: 
  /etc/kafka/connect-distributed.properties 
  /etc/kafka/connect-standalone.properties 
  /etc/tutorial/connect-distributed-for-transform.properties 
 
Completed 
```

- 위와 같은 내용에서 질문에 y로 컨펌해주면 정상적으로 설치 된다. 

## Wrap Up

- 지금까지 가장 기본적인 kafka connector 인 파일 커넥터를 이용하였다. 
- connector은 source, sink 로 데이터를 읽어서 브로커로 전달하는 것을 source라고 하며, 브로커로 부터 메시지를 수신받아 목적지 저장소로 전달하는 것을 sink라고 한다. 
- 또한 메시지를 전송하면서, 메시지를 변환하는 것도 테스트 해 보았다. 
- 기본제공하는 커넥터 이외에 커넥터 허브에서 필요한 커넥터를 다운받아 이용할 수 있다는 것도 알게 되었다. 