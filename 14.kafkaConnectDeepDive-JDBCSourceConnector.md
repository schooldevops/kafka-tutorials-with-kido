# Kafka Connect Deep Dive – JDBC Source Connector

from: https://www.confluent.io/blog/kafka-connect-deep-dive-jdbc-source-connector/#query-based-ingest

- 사람들이 Apache Kafka를 사용하여 가장 일반적인 통합 중 하나는 데이터베이스에서 데이터를 가져오는 것이다. 
- 관계형 데이터베이스들이며 풍부한 이벤트 소스이기 때문이다.
- 데이터베이스의 기존 데이터와 해당 데이터에 대한 변경 사항은 Kafka Topic으로 스트리밍 될 수 있다. 
- 이 이벤트들로 부터 어플리케이션 구동에 사용될 수 있고, 검색 복제본 또는 캐시와 같은 다른 데이터 저장소로 스트리밍하고, 분석을 위해 스토리지로 스트리밍 할 수 있다.

<br/>

- 이 작업을 수행하는 데 사용할 수 있는 옵션과 다양한 유형의 CDC(Change Data Capture) 에 대해 이전에 작성한 적이 있다. 
- 여기에서는 JDBC 커넥터에 대해서 가능한 옵션에 대해서 깊이있게 파 볼 것이다. 
- 이 자체 관리된 시나리오에서 셋업을 어떻게 하고, 트러블 슈팅에 대해서 제공한다. 완젼히 상세한 내용은 문서를 참고하자. https://docs.confluent.io/kafka-connect-jdbc/current/index.html?_ga=2.3480694.757183929.1658272899-2121405869.1653296890&_gac=1.51971803.1656476118.CjwKCAjwzeqVBhAoEiwAOrEmzdjKj0fu85N9diJ-nK9WEOgq6903lhz96C9oW05qqFFRUjiOgfYcCBoCvQIQAvD_BwE

## 소개

- JDBC 커넥터는 Confluent Platform에 포함된다. 그리고 Confluent Hub에서 분리되어 설치될 수 있다. 
- 데이터베이스로 부터 Kafka로 데이터를 가져올(source) 수 있도록 한다. 그리고 데이터 를 Kafka 토픽으로 부터 데이터를 가져가는(Source) 를 수행한다. 
- 
- 대부분의 관계형 데이터베이스는 JDBC Driver 제공한다. 이는 오라클, 마이크로 소프트, SQL Server, DB2, MySQL, Postgres

![kafka](https://cdn.confluent.io/wp-content/uploads/JDBC-connector.png)
